@misc{Acharya:StarAttention:2025,
  author        = {Shantanu Acharya and Fei Jia and Boris Ginsburg},
  title         = {Star Attention: Efficient LLM Inference over Long Sequences},
  year          = {2025},
  archiveprefix = {arXiv},
  eprint        = {2411.17116},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2411.17116},
}

@misc{Behrouz:Titans:2024,
  author        = {Ali Behrouz and Peilin Zhong and Vahab Mirrokni},
  title         = {Titans: Learning to Memorize at Test Time},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2501.00663},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2501.00663},
}

@misc{Dao:FlashAttention:2022,
  author        = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher RÃ©},
  title         = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2205.14135},
  primaryclass  = {cs.LG},
}

@misc{Dao:FlashAttentionV2:2023,
  author        = {Tri Dao},
  title         = {FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2307.08691},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2307.08691},
}

@inproceedings{Dao:Monarch:2022,
  author    = {Dao, Tri and Chen, Beidi and Sohoni, Nimit S and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and Re, Christopher},
  title     = {Monarch: Expressive Structured Matrices for Efficient and Accurate Training},
  year      = {2022},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  month     = {07},
  pages     = {4690--4721},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
}

@inproceedings{Fu:FlashFFTConv:2024,
  author    = {Daniel Y Fu and Hermann Kumbong and Eric Nguyen and Christopher Re},
  title     = {Flash{FFTC}onv: Efficient Convolutions for Long Sequences with Tensor Cores},
  year      = {2024},
  booktitle = {The Twelfth International Conference on Learning Representations},
  url       = {https://openreview.net/forum?id=gPKTTAfYBp},
}

@misc{Gu:Mamba:2024,
  author        = {Albert Gu and Tri Dao},
  title         = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2312.00752},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2312.00752},
}

@misc{Hinton:Dropout:2012,
  author        = {Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
  title         = {Improving neural networks by preventing co-adaptation of feature detectors},
  year          = {2012},
  archiveprefix = {arXiv},
  eprint        = {1207.0580},
  primaryclass  = {cs.NE},
  url           = {https://arxiv.org/abs/1207.0580},
}

@misc{Hu:LoRA:2021,
  author        = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2106.09685},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2106.09685},
}

@misc{Korthikanti:SequenceParallel:2022,
  author        = {Vijay Korthikanti and Jared Casper and Sangkug Lym and Lawrence McAfee and Michael Andersch and Mohammad Shoeybi and Bryan Catanzaro},
  title         = {Reducing Activation Recomputation in Large Transformer Models},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2205.05198},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2205.05198},
}

@inproceedings{Li:SnapKV:2024,
  author    = {Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  title     = {SnapKV: LLM Knows What You are Looking for Before Generation},
  year      = {2024},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
  pages     = {22947--22970},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2024/file/28ab418242603e0f7323e54185d19bde-Paper-Conference.pdf},
  volume    = {37},
}

@inproceedings{Liu:ScalingLawsRoPE:2024,
  author        = {Xiaoran Liu and Hang Yan and Chenxin An and Xipeng Qiu and Dahua Lin},
  title         = {Scaling Laws of Ro{PE}-based Extrapolation},
  year          = {2024},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  archiveprefix = {arXiv},
  eprint        = {2310.05209},
  url           = {https://arxiv.org/abs/2310.05209},
}

@inproceedings{Luong:AttentionNMT:2015,
  author    = {Luong, Thang  and
               Pham, Hieu  and
               Manning, Christopher D.},
  title     = {Effective Approaches to Attention-based Neural Machine Translation},
  year      = {2015},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  address   = {Lisbon, Portugal},
  doi       = {10.18653/v1/D15-1166},
  editor    = {M{\`a}rquez, Llu{\'i}s  and
               Callison-Burch, Chris  and
               Su, Jian},
  month     = {09},
  pages     = {1412--1421},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D15-1166/},
}

@inproceedings{Mikolov:LinguisticRI:2013,
  author    = {Tomas Mikolov and Wen-tau Yih and Geoffrey Zweig},
  title     = {Linguistic Regularities in Continuous Space Word Representations},
  year      = {2013},
  booktitle = {North American Chapter of the Association for Computational Linguistics},
  url       = {https://api.semanticscholar.org/CorpusID:7478738},
}

@inproceedings{Poli:Hyena:2023,
  author    = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y. and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R\'{e}, Christopher},
  title     = {Hyena hierarchy: towards larger convolutional language models},
  year      = {2023},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  articleno = {1164},
  location  = {Honolulu, Hawaii, USA},
  numpages  = {36},
  publisher = {JMLR.org},
  series    = {ICML'23},
}

@misc{Shazeer:MQA:2019,
  author        = {Noam Shazeer},
  title         = {Fast Transformer Decoding: One Write-Head is All You Need},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1911.02150},
  primaryclass  = {cs.NE},
  url           = {https://arxiv.org/abs/1911.02150},
}

@misc{Shoeybi:MegatronLM:2020,
  author        = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  title         = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {1909.08053},
  primaryclass  = {cs.CL},
}

@inproceedings{Sutskever:Seq2seq:2014,
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  title     = {Sequence to Sequence Learning with Neural Networks},
  year      = {2014},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5a18e133cbf9f257297f410bb7eca942-Paper.pdf},
  volume    = {27},
}

@misc{Vaswani:Attention:2023,
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title         = {Attention Is All You Need},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {1706.03762},
  primaryclass  = {cs.CL},
}

@misc{Wang:KvCacheDrop:2025,
  author        = {Guangtao Wang and Shubhangi Upasani and Chen Wu and Darshan Gandhi and Jonathan Li and Changran Hu and Bo Li and Urmish Thakker},
  title         = {LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference},
  year          = {2025},
  archiveprefix = {arXiv},
  eprint        = {2503.08879},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2503.08879},
}
