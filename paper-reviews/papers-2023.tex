\input{papers.tex}

\begin{document}

A total of \total{citenum} papers read in the year.

\section*{Simulators}

\begin{itemize}
    \item \cite{Dahl:Ericsson-VP:2016}:

    Ericsson's experience with virtual platform for pre-silicon software development. The virtual platforms are based on SystemC/TLM. This paper may be used as a showcase of simulation importance.

    \item \cite{Dall:KVM-ARM:2014}:

    The paper describes an early design of KVM/ARM port using the first version of ARM virtualization hardware support. Authors claim to introduce a novel "split-mode" virtualization approach as a way to run hypervisor simultaneously in different CPU privileged modes. This approach is not new though in my opinion. There exists for a long time hypervisors like Simics that use both kernel and user CPU modes to leverage different parts of CPU virtualization. Nevertheless, authors have done a grade work adding ARM support to KVM with generic and rather minimal changes to KVM and Linux kernel. There is an interesting comparison of x86 and ARM virtualization power and performance overheads in the paper. Some of the links in references are also worse looking through later.

    \item \cite{Menard:SystemC-gem5:2017}:

    The article provides a short summary of transaction-based communication in SystemC TLM and in gem5. However the main focus of the paper is on an adapter layer between SystemC TLM and gem5 transactions. The layer provides bi-directional translation for the transaction. The layer was tested on a special-purpose worse-case setup and a couple of more realistic workflows. On the worst-case scenario the layer demonstrated roughly 2x slowdown compared to SystemC only. The integration is claimed to be merged to gem5 official repo.

    \item \cite{Sandberg:FullSpeedAhead:2015}:

    The article describes an approach for fast full-system cycle-accurate simulation to enable efficient hardware and software co-design. The approach address fixed-software limitation of the standard checkpoint-based critical path cycle-accurate simulation approach. First, the authors reduced the porting of functional warming for the samples and introduced error estimation algorithm for cache warming. The error estimation compares best- and worst-case results (run in parallel) to determine whether limited functional warming has significant effect on the results or not. The technique is named Full Speed Ahead. The authors extended the technique with parallel sample execution. The main idea of the parallel Full Speed Ahead approach is to break target workload into samples and execute the samples in parallel while fast-forwarding to the sample boundaries using hardware-supported virtualization. As result, the authors claims to achieve near-native simulation speed while running on a wide enough (in terms of processor cores) host systems.
\end{itemize}

\section*{Processors}
\begin{itemize}
    \item \cite{Gwennap:ARM-Neoverse:2019}:

    A 64 core ARM reference server CPU design similar to Cortex-A76 microarchitecture. Single core performance is not that good but the processor is expectedly much more power efficient. New hardware-based coherence model for i-cache aiming improvement for multiple VM performance. SPEC CPU 2006 is still referenced as a benchmark. Interesting reference to read --- <<Editorial: Intel Hides Behind ICC>> --- MPR claims that ICC must be approximately 30\% faster than GCC (at least for SPEC workloads).

    \item \cite{Demler:Helios:2019}:

    The article describes ARM 64-bit only architecture called Helios. It is a first licensable CPU that combines OOO and multithreaded execution. The main application for the processor are data transfer infrastructure and automotive industry. The processor architecture supports INT8 and FP16 formats that are the current trend for DNN acceleration. On top of INT8 and FP16 the processor can be closely integrated with various accelerators.

    \item \cite{Eyre:DSP-Derby:2001}:

    The article presents digital signal processor (DSP) design and development challenges. The author compares techniques used in DSPs with techniques typical for general-purpose processors. The article emphasizes DSP's requirement for predictable executing speed that is often more important than performance. The requirement is of course harder to fulfill with caches and out-of-order execution. VLIW, SIMD and highly compressed encodings are mentioned as an example of techniques prevailing in DPSs. There is also performance comparison of some DSPs with a superscalar processor that is based on FFT and FIR filter benchmarks.
\end{itemize}

\section*{Compilers}
\begin{itemize}
    \item \cite{Pharr:ispc:2012}:

    The paper presents ispc --- Intel SPMD Program Compiler. A new <<SPMD-on-SIMD>> concept is introduced. The concept seems to be quite reasonable for efficient compilation of SPMD programs for CPUs. A few specific optimizations are described to enable better usage of a CPU's SIMD hardware unit. The project looks like an interesting alternative to hand-written intrinsic-based optimizations. The project is still alive at the moment of writing the note. I can also see a few well familiar names in the list of contributors. As a side note for myself, the paper can be a good reference for \href{https://github.com/yulyugin/mipt-parallel-computing}{<<MIPT parallel computing>> course} if that one ever gets back to live.
\end{itemize}

\section*{Testing}
\begin{itemize}
    \item \cite{Regehr:C-Reduce:2012}:

    As practice shows creating a concise reproducer for a bug is sometimes as complex as fixing the bug. Compiler bugs can be especially hard to report as they are usually observed on real and rather big projects. Even relatively small \href{https://yulyugin.github.io/blog/2022/icc-bug}{compiler bug} reproducers could be hard to analyze and report. Authors of the paper tried to address this important issue by implementing an automatic test case reducers for C.

    \item \cite{Easson:FrobOS:2012}:

    The article describes the ideas behind VMware's minimalistic test operating system FrobOS. Tbe operating system is a great instrument for bare-metal hardware tests. The tooling around the OS also enables nice and fast testing for their hypervisor: simple ways to run port and run the tests between different CPU and hypervisor modes.
\end{itemize}

\section*{Computer Architecture}
\begin{itemize}
    \item \cite{Philips:VLIW:1997}:

    Nice high level overview of VLIW architecture and comparison with typical RISC and CISC designs. Great reference for students to show the basic concepts of CPU architectures even though it is not fully relevant anymore.

    \item \cite{Colwell:Multiflow-VLIW:1987}:

    The paper describes a VLIW architecture developed by Multiflow Corporation. It is one of the VLIW pioneering designs. It is always enjoyable to read old papers about computer architecture trends and ideas; compare the old ideas and expectations with how it turned out now: <<a VLIW machine was expected to provide from ten to thirty times the performance of a more conventional machine built of the same implementation technology>>. A great comments for the citation would be something like: <<if only we would have smart enough compiler>>. Industry practice so far showed that VLIW could be an amazing architecture for some DSPs while a superscalar processor delivers better results for general purpose usage.

    \item \cite{Rajwar:SpeculativeLockElision:2001}:

\end{itemize}

\section*{Autonomous Driving}
\begin{itemize}
    \item \cite{Delmer:Mobileye:2019}:

    The article present an overview of Mobileye existing and in flight processors for advanced-driver-assistance-systems (ADAS). The article also presents a simplistic comparison with Nvidia GPU-based solutions for the area. The main comparison metrics are trillion operations per second (TOPS) and efficiency which is measured as TOPS divided by power consumption. It is an overall nice basic overview of what is happening in the area with cool examples of 360-degree image construction and object classification.
\end{itemize}

\section*{Machine Learning}
\begin{itemize}
    \item \cite{Wheeler:Optical-AI:2019}:

    Optical computing for AI moves closer to commercialization. Startups originating from MIT and Princeton demonstrate interesting optical computing for AI solutions. The systems are targeting convolutional neural networks. 2D filtering is equal to 2D convolution. The approach is aimed to reduce heat and power consumption and of course improve performance simultaneously. One of the main problems so far is optical to electrical and electrical to optical signal conversion. Solutions for the problem seems to be a key for high performance at the moment. The article also contains a link to a scientific paper with more details.

    \item \cite{Gwennap:Centaur-AI:2019}:

    Centaur has developed an x86 processor with deep-learning accelerator --- Ncore. The accelerator has a typical design for the field of application: it's a VLIW architecture relying on traditional SIMD approach with smaller sized vector elements like INT8, BF16, etc. The architecture is 64 times wider that Intel's AVX-512. The x86 cores are alike low-end Skylake cores but it can achieve better neural network performance thanks to the accelerator. Centaur has also constructed a basic software stack that convert TensorFlow to an internal graph format that is later compiled into Ncore assembly - another typical approach for deep-learning accelerators. The article reports that so fat Centaur's software can only handle inference but not training.

    \item \cite{Abadi:TensorFlow:2016}:

    The article describes ideas and reasons that stay behind TensorFlow. Nowadays one can hardly imaging machine learning without TensorFlow. All the accelerators I know about have support for TensorFlow or aim to support it. To represent computations, TensorFlow uses a single dataflow graph where vertices represent operations (atomic units of computations) and edges represent input and output to a vertex. The dataflow graph can be broken into subgraphs and distributed between multiple servers for execution. Basic control flow is supported using \texttt{Switch} and \texttt{Merge} operations. The system provides support for CPUs, GPUs, Google TPUs and allows extensions for other hardware systems.
\end{itemize}

\section*{Virtual Machines}
\begin{itemize}
    \item \cite{Rosenblum:VMM-trends:2005}:

    The article includes a very concise VMM history and some basic techniques like \textit{shadow paging}. Strict focus of the article is on VMware's solutions of that time. There described one interesting paravirtualization idea I didn't know about before --- \textit{ballon process} running inside a Guest OS and communicating with the VMM. The process can allocated a bunch of memory thus forcing the Guest OS to use its knowledge about other processes to page unused memory out. VMM thus can also page out this memory and then reclaim the memory used by the \textit{ballon process} - nice trick.
\end{itemize}

\section*{Binary Translation}
\begin{itemize}
    \item \cite{Altman:BT-Future:2001}:

    The article provides a nice overview of Transmeta Crusoe and IBM DAISY with some insides on the special-purpose underlying hardware. There is an overview of other binary translation systems including LaTTe JVM. There is a clear focus on speculative operations to be performed by Crusoe and DAISY binary translators. One optimization had a bit unclear benefits when I read the article --- the speculative loads performed by DAISY. While reading the article I only thought about the speculative operation as a cache warmup for the later reload. The problem here is that speculative loads could overlap with stores performed before the actual load should occur. To address this issue DAISY uses a \textbf{load-verify} operations that is insert to the location of the original load. The operation reloads the memory value and compares it with the speculatively pre-loaded one. Depending on the result DAISY either continues as normal or enters recovery mode. While writing the note I realized that the load speculation might allow other speculations to be performed and thus the recovery mode is needed to roll them back if the load is overlapped. The article points to the \href{https://patents.google.com/patent/US5758051A}{US5758051A} patent with more details that could be worse to read later.

    \item \cite{Hamayun:StaticBT-VLIW:2013}:

    One of the most confusing papers I read. Unclear why KVM is described in the paper what is the so <<native>> about that approach. I can see two well-known ideas in the paper --- cross-platform compilation and static binary translation. Main focus of the paper is on static binary translation for VLIW CPUs. There is a good description of VLIW weirdness like register dependencies withing the package and delay slots. The authors built a static binary translation that is capable of handling all the tricky VLIW peculiarities.
\end{itemize}

\section*{Performance}
\begin{itemize}
    \item \cite{Yasin:TopDownCPUPerfAnalysis:2014}:

    The paper proposes a top down performance analysis method for superscalar out-of-order processors. The paper discusses a couple of challenges for low-level CPU performance analysis; problems and limitation of the existing methods. The paper suggest a tree-like structure for the analysis. The top level of the hierarchy includes front end, bad speculations, retiring and frontend. Looking at specific part of the hierarchy allows to disregard events happening at other parts thus narrowing down the problem while moving to the leaves of the hierarchy. The author presented few usage examples of the methods and proposed changes to Intel's performance monitoring unit architecture.
\end{itemize}

\end{document}
