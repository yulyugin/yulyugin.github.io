\input{papers.tex}

\begin{document}

A total of \total{citenum} papers read in the year.


\section*{Machine Learning}
\begin{itemize}
    \item \cite{Poli:Hyena:2023}:

    The paper proposes Hyena --- a subquadratic replacement for attention constructed using long convolutions (FFT in particular). Hyena is aiming to solve Transformer scaling problem. Transformers are currently domination in deep learning. However, Transformers are based on the attention operator that exhibits quadratic computational complexity in sequence length which in turn is limiting context available to the model. This fact makes breaking the quadratic barrier a key step forwards new deep learning capabilities.

    \item \cite{Fu:FlashFFTConv:2024}:

    FlashFFTConv algorithm proposed in the paper tries to address major bottlenecks related to convolutional models with long filters --- poor utilization of specialized matrix multiplication units and expensive I/O between layers of memory hierarchy. The major bottleneck is caused by the Fast Fourier Transform. Convolutional models lag behind the most Transformers even though FFT allows to run in O(N log N) instead of  O(N^2) common Transformers. FlashFFTConv uses a matrix decomposition that computes FFT using matrix multiplication units and enables kernel fusion for long sequences that in turn allows to reduce the expensive I/O operations.

    \item \cite{Dao:Monarch:2022}:

    The paper proposes a class of hardware efficient Monarch matrices. The matrices are proposed as a replacement for dense weight matrices in order to reduce compute/memory requirements of large neural networks. Monarch matrices are block diagonal matrices that can represent many fast transforms such as Fourier or convolution. The authors show that dense-to-sparse conversion has an analytically optimal solution. A Monarch matrix is described by $2n\sqrt{n}$ parameters instead $n^2$ for dense matrix. Monarch parametrization allows efficient utilization of batch matrix multiply units available in contemporary hardware such as GPUs. The parametrization allows to achieve up to $2x$ speedup compared to dense matrix multiply. Algorithm proposed in the paper allows to fine-tune a pre-trained model into a model with Monarch weight matrices. The work presented in the paper is a step towards doing structural matrices more practical.

    \item \cite{Shoeybi:MegatronLM:2020}:

    The article describes techniques for training large transformer models that is based on intra-layer model parallel approach. The approach doesn't require a complier or library changes and is only based on PyTorch level changes to a model. The approach is demonstrated by increasing number of parameters for existing transformer models. The main idea of the technique is to partition GEMM in a way that allows non-linear functions to be applied to each partition independently. The technique is aimed to reduce number of communications (reductions) introduces py partitioning. The technique is orthogonal to data parallelism thus allowing combination of the two. The approach has demonstrated near linear scaling.

    \item \cite{Rouhani:Microscaling:2023}:

    The paper investigates performance on narrow-bit data formats for machine learning applications. Narrow-bit data types are aimed to reduce computational and storage costs of the applications. Mixed precision calculations are considered in the paper: GeMMs performed in the narrow-bit data format and all the vector calculations are preformed in Bfloat16 or FP32. The authors showed that both model training and model inference performance doesn't downgrade that much when going to narrow-bit formats. INT8 almost matched the baseline FP32 in experiments done by the paper. The results demonstrated that training with MXFP4 weights and MXFP6 activations incurs only a minor penalty in model performance loss.

    \item \cite{Devlin:Bert:2019}:

    The paper presents BERT --- language model based on multi-layer bidirectional transformer. BERT training consists of two parts: pre-training and fine-tuning. Pre-training is done on unlabeled data. For fine-tuning the model is first initialized with pre-trained parameters and then all the parameters are fine-tuned using labeled data from the target tasks. Fine-tuning is relatively inexpensive compared to pre-training. BERT pre-training is done using "masked LM" approach --- random words in a text sequenced are masked and the language model is trained to predict them correctly thus allowing to obtain bidirectional model. To understand dependencies between sentences, the model is trained on labeled sentence pairs that are related (second sentence is following the first one in the original text) and labeled as "next" in 50\% cases and not related (second sentence is random) and labeled as "not next" in the other 50\% cases.
\end{itemize}

\end{document}