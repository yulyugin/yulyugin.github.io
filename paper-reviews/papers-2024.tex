\input{papers.tex}

\begin{document}

A total of \total{citenum} papers read in the year.


\section*{Machine Learning}
\begin{itemize}
    \item \cite{Poli:Hyena:2023}:

    The paper proposes Hyena --- a subquadratic replacement for attention constructed using long convolutions (FFT in particular). Hyena is aiming to solve Transformer scaling problem. Transformers are currently domination in deep learning. However, Transformers are based on the attention operator that exhibits quadratic computational complexity in sequence length which in turn is limiting context available to the model. This fact makes breaking the quadratic barrier a key step forwards new deep learning capabilities.
\end{itemize}

\end{document}