\input{papers.tex}

\begin{document}

A total of \total{citenum} papers read in the year.

\section*{Machine Learning}
\begin{itemize}
    \item \cite{DeepSeekAI:2025:DeepSeekV3}:

    The report presents DeepSeek-V3 model architecture. The key difference from the Transformer architecture are: using Multi-Head Latent Attention (MLA) instead of MHA; using Mixture of Experts (MoE) instead of Feed Forward Network. DeepSeek-V3 MoE consists of shared and routed experts. Every token will run through the shared experts and select a few routed once. Each token may potentially end up using completely different routed experts. MLA implementation is aimed to reduce KV-cache usage by having a single shared cache for all heads. It may sound similar to Grouped Query Attention (GQA) but instead of compressing information from multiple heads into one, MLA moves per-head key and value information to per-head query thus only having the common for all heads information in keys and values. The authors also introduce Multi-Token Prediction (MTP) module that is used for training in the original work but can potentially be used for speculatively predicting several tokens in one run of the model.

    \item \cite{Behrouz:Titans:2024}:

    The authors present Titans --- a new machine learning model architecture that incorporates long-term memory with forgetting in order to be able to handle very large context windows. A forgetting mechanism is added to the model to avoid memory overflows. Long-term memory is added as a context to the current input sequence.

    \item \cite{Gu:Mamba:2024}:

    The article presents Mamba model architecture that is aimed to address Transformer's computational inefficiency on long sequence lengths. The authors claim that the key weakness of the existing subquadratic-time architectures is inability to perform context-based reasoning. The mentioned problem is addressed by letting the structured state space model (SSM) parameters be functions of the input thus allowing the model to selectively propagate and forget information along the sequence length based on the current token. Mamba is based on these selective SSMs and requires neither attention nor MLP blocks. As results Mamba inference can scale linearly with sequence length. The key ability of Mamba is selectivity --- context-aware way to focus on or filter out inputs info sequential state. The authors achieve efficient utilization of the existing hardware through standard techniques: kernel fusion, parallel scan and recomputation.

    \item \cite{Hu:LoRA:2021}:

    The article presents LoRA (Low-Rank Adaptation) technique for large language models. The technique is aimed to speedup fine-tuning and inference of the fine-tuned model. Traditionally, fine-tuning is used to update weights of a model pre-trained on a large set of generic data in order to specialize the model to handle a specific task. Basically it is a retraining of a generic model on a dataset specific for a desired task. With large language model it poses an obvious issue --- the fine-tuned model has the same number of parameters as the original model which makes the retraining task expensive and long. The main idea of LoRA is to freeze the pre-trained model weights and inject additional trainable rank decomposition matrices into the model in order to greatly reduce the number of trainable parameters. This approach allows easy switch of the model in production environment --- only the smaller additional matrices need to be switched. It is also possible to merge the trainable matrices into the original pre-trained model weights to avoid introducing extra latency during inference. The LoRA matrices consist of down projection from the original dimension to a small LoRA rank (the authors showed that even rank of two can be efficient) and up projection back to the original dimension. The high level idea is that the LoRA rank will record information specific for a desired task. The paper shows that it is preferable to adopt more weight matrices than to adopt a single weight matrix with a larger rank.

    \item \cite{Mattson:MLPerfTraining:2020}:

    The paper present MLPerf training benchmark and describes challenges that are rather unique for machine learning workloads: stochastic nature of the training task; necessity to train to the completion to be able to compare model quality. For example, lower data precision from FP32 to BF16 will definitely improve training throughput but it may require a lot more iterations to complete the training task (achieve similar model quality). The benchmark includes a set of model architectures aimed to solve the most common machine learning tasks including image classification, object detection, translation (both transformer and RNN), etc.

    \item \cite{Korthikanti:SequenceParallel:2022}:

    The paper presents sequence parallel and selective activation recomputation techniques. The techniques are aimed to accelerate training of large Transformer models by reducing activation recomputation. The sequence parallel technique extends Megatron-LM idea by running previously sequential parts in sequence parallel manner. Basically, the Transformer layer is split into four interchanging tensor parallel and sequence parallel block. GEMMs run using tensor parallel while layer norm and dropout run using sequence parallel. Interesting to note that sequence parallel does not add any extra communication overhead because communication required for tensor parallel already does all operations needed for sequence parallel part. Ring all-reduce (required for tensor parallel) is composed of reduce-scatter and all-gather (both required for sequence parallel). One downside is that execution of reduce-scatter and all-gather combined is slower than all-reduce alone. In general, the paper can be used as a great source to get an idea of different model parallelization techniques.

    \item \cite{Wang:KvCacheDrop:2025}:

    The paper describes an idea to optimize KV-cache size and attention calculations performed during decode (token generation) stage. The compressed version of KV-cache created during prefill (cache generation) stage consists of K and V results for fixed number of initial (first in input sequence) tokens, fixed number of recent tokens, last token and top-k of all the tokens in between initial initial and recent. The general idea is that initial and recent token are important and top-k selection can keep all the other important information while dropping other. Only recent tokens and last token in the KV-cache are getting updated during decode stage. Recent tokens acting like a ring buffer. Initial tokens and top-k stay the same. Thus KV-cache size always stays the same.

    \item \cite{Liu:ScalingLawsRoPE:2024}:

    The paper describes how Rotary Position Embedding (RoPE) is used by Transformer models to discern positional information in the input sequence. The authors studied extrapolation capabilities of Large Language Models based on RoPE with focus on modifications to rotary base. The paper proposed "Scaling Laws of RoPE-based Extrapolation", a framework to describe relations between the extrapolation performance and rotary base values as well as tunning context length.

    \item \cite{Luong:AttentionNMT:2015}:

    The authors present earlier attention-based models for machine translation tasks. The paper considers global and local attention approaches. The global approach attends to all words in the input sequence while the local one only looks at a subset of input words at a time. The article proposes and compares several mechanisms to decide what words in input sequence to attend to for the local attention approach. The resulting model yields a new state-of-the-art result in English to German translation task.

    \item \cite{Chen:AdaComp:2018}:

    The paper presents Adaptive Residual Gradient Compression (AdaComp) scheme that is based on Local Selection (LS) of gradient residuals. The main difference from LS is that AdaComp automatically adjust compression rate depending local configuration that includes number of learners, neural network architecture, layer types, mini-batch sizes and other parameters. Thus AdaComp provides soft compression threshold (allowing to self-adjust the compression rate) compared to LS that has the threshold as a fix parameter. The authors managed to achieve $~200X$ compression rate for fully-connected and recurrent layers, and $~40X$ compression rate for convolutional layers.

    \item \cite{Sutskever:Seq2seq:2014}:

    The paper presents LSTM-based (Long Short-Term Memory) sequence to sequence model for machine translation. The main idea is to first translate \textit{reversed} input sequence to an intermediate fixed dimensioned vector representation and then translate the acquired representation to the target language. The input sequence is reversed to get shorter distance between input and target words during training. The resulting model performed better than advanced Statistical Machine Translation (SMT) system.

    \item \cite{Mudigere:MetaDLRMTraining:2022}:

    The paper present Neo (software) and ZionEX (hardware) systems co-designed for Deep Learning Recommendation Model (DLRM) training. The very interesting feature of the DLRMs is that the models use lots of data-intensive embedding operators compared to mostly compute-intensive operations in conventional Deep Neural Networks (DNNs). Existing software and hardware solutions are typically targeting DNNs. Such frameworks are optimized for compute-intense workloads and miss critical optimizations for data-intense embedding operators. The authors' solution includes 4D parallelism (table-wise, row-wise, column-wise and data), high-performance embedding computation through fused kernels and special hardware design to achieve better communication and memory access speed.

    \item \cite{Li:SnapKV:2024}:

    The paper presents KV cache compression technique called SnapKV. The compression is achieved by selecting important KV positions for each attention head. The key differences from other techniques are: SnapKV uses an observation windows consisting of several last tokens instead of just one; SnapKV uses pooling instead naive top feature results for top-k selection. The technique is based on an observation that each attention head in a model consistently focuses on specific prompt attention features. SnapKV allows to KV cache memory usage and compute demands during decoding but it does not affect prefill.

    \item \cite{Acharya:StarAttention:2025}:

    The paper presents distributed block-sparse approximate attention algorithm called Star Attention. The algorithm is broken into two phases. In the first phase, each node calculates block-wise local attention with context window segmented into blocks and each block prefixed with an anchor (first context block). In the seconds phase, query and local attention results are collected and accumulated to a global attention approximation by one selected node. The authors demonstrate that Star Attention can achieve substantial speedups without significant model accuracy degradation. Block size variation allows for speed/accuracy balancing.
\end{itemize}

\section*{Performance}
\begin{itemize}
    \item \cite{Jacobs:DataBasePerformance:2009}:

    The article highlights optimization challenges that database developers face while working on big data analysis. The author demonstrates performance drawbacks of reading large databases for different kind of analyses. The article is largely focused on efficient usage of hardware capabilities: random vs. sequential access to storage devices. There is a good diagram that compares random and sequential accesses of hard drives, SSDs and DRAM. The author also shows few possible database architecture focusing on possible read requests that are supposed to be serviced by the database. There is also a good definition of "big data" in the very end of the paper.

    \item \cite{Gwennap:IntelHidesBehindICC:2017}:

    The article highlights how Intel uses ICC compiler and library optimizations to submit higher SPEC CPU scores. The author emphasizes that it is important to use GCC --- an open sources, harder to manipulate compiler --- to achieve apple to apple comparison of different vendors' processors.
\end{itemize}

\end{document}
