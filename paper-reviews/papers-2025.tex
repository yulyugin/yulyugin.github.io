\input{papers.tex}

\begin{document}

A total of \total{citenum} papers read in the year.

\section*{Machine Learning}
\begin{itemize}
    \item \cite{DeepSeekAI:2025:DeepSeekV3}:

    The report presents DeepSeek-V3 model architecture. The key difference from the Transformer architecture are: using Multi-Head Latent Attention (MLA) instead of MHA; using Mixture of Experts (MoE) instead of Feed Forward Network. DeepSeek-V3 MoE consists of shared and routed experts. Every token will run through the shared experts and select a few routed once. Each token may potentially end up using completely different routed experts. MLA implementation is aimed to reduce KV-cache usage by having a single shared cache for all heads. It may sound similar to Grouped Query Attention (GQA) but instead of compressing information from multiple heads into one, MLA moves per-head key and value information to per-head query thus only having the common for all heads information in keys and values. The authors also introduce Multi-Token Prediction (MTP) module that is used for training in the original work but can potentially be used for speculatively predicting several tokens in one run of the model.

    \item \cite{Behrouz:Titans:2024}:

    The authors present Titans --- a new machine learning model architecture that incorporates long-term memory with forgetting in order to be able to handle very large context windows. A forgetting mechanism is added to the model to avoid memory overflows. Long-term memory is added as a context to the current input sequence.

    \item \cite{Gu:Mamba:2024}:

    The article presents Mamba model architecture that is aimed to address Transformer's computational inefficiency on long sequence lengths. The authors claim that the key weakness of the existing subquadratic-time architectures is inability to perform context-based reasoning. The mentioned problem is addressed by letting the structured state space model (SSM) parameters be functions of the input thus allowing the model to selectively propagate and forget information along the sequence length based on the current token. Mamba is based on these selective SSMs and requires neither attention nor MLP blocks. As results Mamba inference can scale linearly with sequence length. The key ability of Mamba is selectivity --- context-aware way to focus on or filter out inputs info sequential state. The authors achieve efficient utilization of the existing hardware through standard techniques: kernel fusion, parallel scan and recomputation.
\end{itemize}

\end{document}
